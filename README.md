# Empirical-Study-of-Adversarial-Attacks-on-ESC-
<b>This code can be used to perform analysis of impact of adversarial attacks on environmental sound classification systems. A study is performed with seven state-of-the-art deep models well known to perform classification for computer vision related tasks. 
The empirical study includes analysis of fooling rate, transferability of adversarial examples and L2 peturbation distance as a measure to create the adversarial sample
by the deep model. This study is performed on three benchmark ESC datasets: ESC-10, Urban Sound 8K and DCASE 2019 Challenge Task-1(A) dataset.

<b><font color="red">Requirements: <br>
Pytorch== 1.5<br>
Python== 3.8 <br>
Cuda Toolkit==10.2<br> 
Operating System: POP! OS 20.10 <br>
GPU Confugration: Persistence-M 

